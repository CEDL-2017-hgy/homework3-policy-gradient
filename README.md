# Homework2-Policy Gradient

In this homework, you will use a neural network to learn a parameterize policy that can select action without consulting a value function. A value function may still be used to learn the policy weights, but is not required for action selection. 

There are some advantage of the policy-based algorithms:

- Policy-based methods also offer useful ways of dealing with continuous action spaces
- For some tasks, the policy function is simpler and thus easier to approximate.


## Introduction

We will use ```CartPole-v0``` as environment in this homework. The following gif is the visualization of the CartPole: 

<img src="https://cloud.githubusercontent.com/assets/7057863/19025154/dd94466c-8946-11e6-977f-2db4ce478cf3.gif" width="400" height="200" />

For further description, please see [here](https://gym.openai.com/envs/CartPole-v0)

## Setup
- Python 3.5.3
- OpenAI gym
- **tensorflow**
- numpy
- matplotlib
- ipython
All the codes you need to modified are in ```Lab3-PG.ipynb```. 

We encourage you to install [Anaconda](https://www.anaconda.com/download/) or [Miniconda](https://conda.io/miniconda.html) in your laptop to avoid tedious dependencies problem.

**for lazy people:**

```
conda env create -f environment.yml
source activate cedl
# deactivate when you want to leaving the environment
source deactivate cedl
```

## TODO

- []
- []
- []
- []



## Other
- Some of the codes are credited to Yen-Chen Lin :smile:
- Office hour 2-3 pm in 資電館711 with [Yuan-Hong Liao](https://andrewliao11.github.io).

